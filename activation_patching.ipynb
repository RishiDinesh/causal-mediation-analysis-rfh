{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4cb8ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/traces.jsonl\", \"r\") as f:\n",
    "    traces = [json.loads(line) for line in f.readlines()]\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1455c31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_MODEL_ALIAS_PATH = {\n",
    "    'llama-8B': r\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    'qwen-7B': r\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    'qwen-1p5B': r\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72063c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "def mps_cleanup():\n",
    "    # Drop Python refs\n",
    "    gc.collect()\n",
    "    # Free MPS allocator cache (CUDA equivalent is torch.cuda.empty_cache())\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71f4a2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishidinesh/Projects/causal-mediation-analysis-rfh/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    \n",
    "model = HookedTransformer.from_pretrained_no_processing(\n",
    "    D_MODEL_ALIAS_PATH['qwen-1p5B'],\n",
    "    dtype=torch.bfloat16,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e019c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'qwen-1p5B',\n",
       " 'unique_id': 'test/intermediate_algebra/1994.json',\n",
       " 'problem': 'Define\\n\\\\[p = \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{k^2} \\\\quad \\\\text{and} \\\\quad q = \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{k^3}.\\\\]Find a way to write\\n\\\\[\\\\sum_{j = 1}^\\\\infty \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\]in terms of $p$ and $q.$',\n",
       " 'response_withR': \"<｜begin▁of▁sentence｜>Define\\n\\\\[p = \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{k^2} \\\\quad \\\\text{and} \\\\quad q = \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{k^3}.\\\\]Find a way to write\\n\\\\[\\\\sum_{j = 1}^\\\\infty \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\]in terms of $p$ and $q.$ Please reason step by step, and put your final answer within \\\\boxed{}. <think>\\nOkay, so I have this problem where I need to find a way to express the double sum \\\\(\\\\sum_{j = 1}^\\\\infty \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\) in terms of \\\\(p\\\\) and \\\\(q\\\\), where \\\\(p = \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{k^2}\\\\) and \\\\(q = \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{k^3}\\\\). Hmm, okay. I remember that these are known as the Riemann zeta functions. So, \\\\(p = \\\\zeta(2)\\\\) and \\\\(q = \\\\zeta(3)\\\\). \\n\\nBut how do I relate this double sum to \\\\(\\\\zeta(2)\\\\) and \\\\(\\\\zeta(3)\\\\)? I know that sometimes you can use integral representations or some kind of series manipulation to express these sums. Maybe I can find a way to express the double sum as a single sum or a combination of \\\\(p\\\\) and \\\\(q\\\\).\\n\\nLet me think about the structure of the double sum: it's over all positive integers \\\\(j\\\\) and \\\\(k\\\\), and for each pair \\\\((j, k)\\\\), we're taking the reciprocal of \\\\((j + k)^3\\\\). That seems a bit tricky because the denominator isn't just a function of \\\\(j\\\\) or \\\\(k\\\\) alone, but a function of their sum. \\n\\nPerhaps I can change variables or find a substitution that simplifies the expression. Let me consider changing variables such that \\\\(n = j + k\\\\), and then express \\\\(j\\\\) or \\\\(k\\\\) in terms of \\\\(n\\\\). But since \\\\(j\\\\) and \\\\(k\\\\) are both positive integers, for a fixed \\\\(n\\\\), \\\\(j\\\\) can range from 1 to \\\\(n - 1\\\\), and \\\\(k = n - j\\\\). So, maybe I can rewrite the double sum as a single sum over \\\\(n\\\\), with coefficients depending on \\\\(j\\\\). \\n\\nSo, let's try that. Let \\\\(n = j + k\\\\), then for each \\\\(n\\\\), \\\\(j\\\\) can be from 1 to \\\\(n - 1\\\\), and \\\\(k = n - j\\\\). So, the double sum becomes:\\n\\n\\\\[\\n\\\\sum_{n = 2}^\\\\infty \\\\sum_{j = 1}^{n - 1} \\\\frac{1}{n^3}\\n\\\\]\\n\\nBecause \\\\(j + k = n\\\\), so \\\\((j + k)^3 = n^3\\\\). So, now we have a sum over \\\\(n\\\\) from 2 to infinity, and for each \\\\(n\\\\), a sum over \\\\(j\\\\) from 1 to \\\\(n - 1\\\\), each term being \\\\(\\\\frac{1}{n^3}\\\\). \\n\\nTherefore, the inner sum is just \\\\(\\\\sum_{j = 1}^{n - 1} \\\\frac{1}{n^3}\\\\), which is \\\\(\\\\frac{1}{n^3} \\\\times (n - 1)\\\\), since there are \\\\(n - 1\\\\) terms. So, that simplifies to:\\n\\n\\\\[\\n\\\\sum_{n = 2}^\\\\infty \\\\frac{n - 1}{n^3}\\n\\\\]\\n\\nHmm, okay, so now we have a single sum from \\\\(n = 2\\\\) to infinity. Let's see if we can simplify this further. Maybe we can split the fraction:\\n\\n\\\\[\\n\\\\frac{n - 1}{n^3} = \\\\frac{n}{n^3} - \\\\frac{1}{n^3} = \\\\frac{1}{n^2} - \\\\frac{1}{n^3}\\n\\\\]\\n\\nOh, that's nice! So, substituting back, we have:\\n\\n\\\\[\\n\\\\sum_{n = 2}^\\\\infty \\\\left( \\\\frac{1}{n^2} - \\\\frac{1}{n^3} \\\\right ) = \\\\sum_{n = 2}^\\\\infty \\\\frac{1}{n^2} - \\\\sum_{n = 2}^\\\\infty \\\\frac{1}{n^3}\\n\\\\]\\n\\nNow, let's relate this to the given \\\\(p\\\\) and \\\\(q\\\\). Remember, \\\\(p = \\\\sum_{n = 1}^\\\\infty \\\\frac{1}{n^2}\\\\) and \\\\(q = \\\\sum_{n = 1}^\\\\infty \\\\frac{1}{n^3}\\\\). So, the sums from \\\\(n = 2\\\\) to infinity can be expressed in terms of \\\\(p\\\\) and \\\\(q\\\\) as follows:\\n\\n\\\\[\\n\\\\sum_{n = 2}^\\\\infty \\\\frac{1}{n^2} = p - \\\\frac{1}{1^2} = p - 1\\n\\\\]\\n\\\\[\\n\\\\sum_{n = 2}^\\\\infty \\\\frac{1}{n^3} = q - \\\\frac{1}{1^3} = q - 1\\n\\\\]\\n\\nTherefore, substituting back into our expression, we have:\\n\\n\\\\[\\n(p - 1) - (q - 1) = p - 1 - q + 1 = p - q\\n\\\\]\\n\\nWait a minute, that seems too simple. So, is the double sum equal to \\\\(p - q\\\\)? Let me verify that.\\n\\nSo, starting from the double sum:\\n\\n\\\\[\\n\\\\sum_{j = 1}^\\\\infty \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{(j + k)^3}\\n\\\\]\\n\\nWe changed variables to \\\\(n = j + k\\\\), and for each \\\\(n\\\\), \\\\(j\\\\) ranges from 1 to \\\\(n - 1\\\\). Then, the double sum becomes:\\n\\n\\\\[\\n\\\\sum_{n = 2}^\\\\infty \\\\sum_{j = 1}^{n - 1} \\\\frac{1}{n^3}\\n\\\\]\\n\\nWhich is:\\n\\n\\\\[\\n\\\\sum_{n = 2}^\\\\infty \\\\frac{n - 1}{n^3} = \\\\sum_{n = 2}^\\\\infty \\\\left( \\\\frac{1}{n^2} - \\\\frac{1}{n^3} \\\\right )\\n\\\\]\\n\\nThen, we split it into two sums:\\n\\n\\\\[\\n\\\\left( \\\\sum_{n = 2}^\\\\infty \\\\frac{1}{n^2} \\\\right ) - \\\\left( \\\\sum_{n = 2}^\\\\infty \\\\frac{1}{n^3} \\\\right )\\n\\\\]\\n\\nExpressed in terms of \\\\(p\\\\) and \\\\(q\\\\), this is:\\n\\n\\\\[\\n(p - 1) - (q - 1) = p - q\\n\\\\]\\n\\nSo, that seems correct. But let me think again: is there a mistake here? Because when we changed variables, we assumed that \\\\(j\\\\) and \\\\(k\\\\) are independent, which they are, but when we express the sum as a single sum over \\\\(n\\\\), we have to account for the fact that each term corresponds to a unique pair \\\\((j, k)\\\\). \\n\\nWait, actually, when \\\\(n = j + k\\\\), for each \\\\(n\\\\), the number of pairs \\\\((j, k)\\\\) with \\\\(j + k = n\\\\) is \\\\(n - 1\\\\), which is what we used. So, that seems correct. So, the sum is equal to the sum over \\\\(n\\\\) from 2 to infinity of \\\\(\\\\frac{n - 1}{n^3}\\\\), which is equal to \\\\(p - q\\\\). So, that seems right.\\n\\nBut let me test with a smaller case. Suppose we have a finite version of the double sum, say up to \\\\(j = 2\\\\) and \\\\(k = 2\\\\). Then, the sum would be:\\n\\n\\\\[\\n\\\\frac{1}{(1 + 1)^3} + \\\\frac{1}{(1 + 2)^3} + \\\\frac{1}{(2 + 1)^3} + \\\\frac{1}{(2 + 2)^3}\\n\\\\]\\n\\nWhich is:\\n\\n\\\\[\\n\\\\frac{1}{8} + \\\\frac{1}{27} + \\\\frac{1}{27} + \\\\frac{1}{64}\\n\\\\]\\n\\nCalculating that:\\n\\n\\\\[\\n\\\\frac{1}{8} + \\\\frac{2}{27} + \\\\frac{1}{64} = \\\\frac{8}{64} + \\\\frac{16}{64} + \\\\frac{1}{64} = \\\\frac{25}{64} \\\\approx 0.390625\\n\\\\]\\n\\nNow, let's compute \\\\(p - q\\\\) for finite \\\\(p\\\\) and \\\\(q\\\\). Let's take, say, \\\\(p\\\\) as \\\\(\\\\sum_{k=1}^2 \\\\frac{1}{k^2} = 1 + \\\\frac{1}{4} = 1.25\\\\) and \\\\(q = \\\\sum_{k=1}^2 \\\\frac{1}{k^3} = 1 + \\\\frac{1}{8} = 1.125\\\\). Then, \\\\(p - q = 1.25 - 1.125 = 0.125\\\\). \\n\\nBut our finite sum was approximately 0.390625, which is not equal to 0.125. Hmm, so this suggests that my initial approach is incorrect. Wait, but that might be because the finite case isn't large enough. Let me try with larger \\\\(n\\\\).\\n\\nAlternatively, maybe I made a mistake in the change of variables. Let's think again.\\n\\nThe double sum is:\\n\\n\\\\[\\n\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3}\\n\\\\]\\n\\nLet me swap the order of summation. Maybe I can write this as:\\n\\n\\\\[\\n\\\\sum_{n=2}^\\\\infty \\\\sum_{j=1}^{n - 1} \\\\frac{1}{n^3}\\n\\\\]\\n\\nWhich is the same as before. So, that part is correct. Then, the sum over \\\\(n\\\\) is from 2 to infinity, and for each \\\\(n\\\\), the inner sum is \\\\(\\\\frac{n - 1}{n^3}\\\\). So, that leads us to the expression \\\\(p - q\\\\).\\n\\nBut in the finite case, it doesn't hold. So, maybe the finite case is not good enough, but the infinite case is correct. Let me think about the finite case again.\\n\\nSuppose we take \\\\(p = 1 + \\\\frac{1}{4} = 1.25\\\\) and \\\\(q = 1 + \\\\frac{1}{8} = 1.125\\\\). Then, \\\\(p - q = 0.125\\\\). But the double sum up to \\\\(j = 2\\\\) and \\\\(k = 2\\\\) was 0.390625, which is larger than 0.125.\\n\\nBut if we take the double sum up to \\\\(j = 2\\\\) and \\\\(k = 2\\\\), it's 0.390625, which is equal to \\\\(\\\\frac{1}{8} + \\\\frac{1}{27} + \\\\frac{1}{27} + \\\\frac{1}{64}\\\\). Hmm, if we calculate that, 0.125 is \\\\(1/8\\\\), but the rest are larger. So, that's not the case. \\n\\nWait, perhaps my approach is wrong because when I change variables, I have to consider the multiplicity of each term. Wait, no, because each term in the double sum is unique for each pair \\\\((j, k)\\\\). So, when I change variables to \\\\(n = j + k\\\\), each term is counted exactly once for each \\\\(n\\\\), and the number of terms for each \\\\(n\\\\) is \\\\(n - 1\\\\). So, that should be correct.\\n\\nWait, but perhaps the issue is that the change of variables isn't accounting for the fact that \\\\(j\\\\) and \\\\(k\\\\) are symmetric. So, in other words, the sum over \\\\(j\\\\) and \\\\(k\\\\) is symmetric, so maybe I should only consider \\\\(n\\\\) from 2 to infinity, but in my calculation, I have already considered all pairs where \\\\(j\\\\) and \\\\(k\\\\) are positive integers. So, that seems correct.\\n\\nAlternatively, perhaps I can think of the double sum as the convolution of two sequences. The double sum is equivalent to the convolution of the sequence \\\\(a_j = \\\\frac{1}{j^3}\\\\) with itself, evaluated at \\\\(n = 2\\\\). Wait, no, the convolution is usually defined as \\\\(\\\\sum_{j=1}^\\\\infty a_j b_{n - j}\\\\), but in this case, since we have \\\\(j + k = n\\\\), so it's similar to a convolution with \\\\(a_j = \\\\frac{1}{j^3}\\\\) and \\\\(b_k = \\\\frac{1}{k^3}\\\\). So, the double sum is the convolution at \\\\(n = 2\\\\), but since it's a double sum, maybe it's similar to the square of the sequence \\\\(a_j\\\\) evaluated at \\\\(n = 2\\\\). But I'm not sure.\\n\\nWait, perhaps another way is to consider that the double sum is equal to \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\), which can be written as \\\\(\\\\sum_{n=2}^\\\\infty \\\\sum_{j=1}^{n - 1} \\\\frac{1}{n^3}\\\\), as before. So, that seems consistent.\\n\\nAlternatively, maybe we can relate this to the square of the zeta function or something like that. But I'm not sure. Wait, maybe we can express this double sum in terms of the square of the sum, but that might not directly help.\\n\\nWait, another idea: perhaps we can write the double sum as \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3} = \\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\), and then interchange the order of summation, but I don't see how that would help.\\n\\nAlternatively, maybe I can write the double sum as \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3} = \\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\), which is the same as before. So, perhaps the initial approach is correct, and the answer is indeed \\\\(p - q\\\\). But in the finite case, it's not matching. So, maybe in the finite case, it's different.\\n\\nWait, let me try with \\\\(p = \\\\sum_{k=1}^3 \\\\frac{1}{k^2} = 1 + 1/4 + 1/9 = 13/9 \\\\approx 1.444\\\\) and \\\\(q = \\\\sum_{k=1}^3 \\\\frac{1}{k^3} = 1 + 1/8 + 1/27 = 133/108 \\\\approx 1.234\\\\). Then, \\\\(p - q = 13/9 - 133/108 = (156 - 133)/108 = 23/108 \\\\approx 0.213\\\\). Now, let's compute the double sum up to \\\\(j = 3\\\\) and \\\\(k = 3\\\\):\\n\\nCompute:\\n\\n\\\\[\\n\\\\frac{1}{(1 + 1)^3} + \\\\frac{1}{(1 + 2)^3} + \\\\frac{1}{(1 + 3)^3} + \\\\frac{1}{(2 + 1)^3} + \\\\frac{1}{(2 + 2)^3} + \\\\frac{1}{(2 + 3)^3} + \\\\frac{1}{(3 + 1)^3} + \\\\frac{1}{(3 + 2)^3} + \\\\frac{1}{(3 + 3)^3}\\n\\\\]\\n\\nCalculating each term:\\n\\n\\\\[\\n\\\\frac{1}{8} + \\\\frac{1}{27} + \\\\frac{1}{64} + \\\\frac{1}{27} + \\\\frac{1}{64} + \\\\frac{1}{125} + \\\\frac{1}{64} + \\\\frac{1}{125} + \\\\frac{1}{216}\\n\\\\]\\n\\nAdding them up:\\n\\nFirst, \\\\(\\\\frac{1}{8} = 0.125\\\\)\\n\\nThen, \\\\(3 \\\\times \\\\frac{1}{27} = \\\\frac{1}{9} \\\\approx 0.1111\\\\)\\n\\nNext, \\\\(3 \\\\times \\\\frac{1}{64} = \\\\frac{3}{64} \\\\approx 0.0469\\\\)\\n\\nThen, \\\\(2 \\\\times \\\\frac{1}{125} = \\\\frac{2}{125} = 0.016\\\\)\\n\\nFinally, \\\\(\\\\frac{1}{216} \\\\approx 0.00463\\\\)\\n\\nAdding all together: 0.125 + 0.1111 + 0.0469 + 0.016 + 0.00463 ≈ 0.2996\\n\\nWhich is approximately 0.3, which is higher than \\\\(p - q \\\\approx 0.213\\\\). So, this suggests that even in the finite case, the double sum isn't equal to \\\\(p - q\\\\). So, perhaps my initial approach is incorrect.\\n\\nWait, but in the finite case, the double sum is equal to \\\\(\\\\frac{p - q + something}{something}\\\\). Hmm, but in the infinite case, it's equal to \\\\(p - q\\\\), but in the finite case, it's not. So, perhaps the initial approach is wrong.\\n\\nWait, another idea: Maybe I can express the double sum as \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3} = \\\\sum_{n=2}^\\\\infty \\\\sum_{j=1}^{n - 1} \\\\frac{1}{n^3} = \\\\sum_{n=2}^\\\\infty \\\\frac{n - 1}{n^3}\\\\), which is correct, and then as we saw, this is \\\\(p - q\\\\). But in the finite case, it's not. So, maybe in the finite case, the double sum is not equal to \\\\(p - q\\\\), but in the infinite case, it is. So, perhaps the answer is \\\\(p - q\\\\), but in the finite case, it's different.\\n\\nBut wait, in the finite case, we have:\\n\\n\\\\[\\n\\\\sum_{j=1}^3 \\\\sum_{k=1}^3 \\\\frac{1}{(j + k)^3} = \\\\frac{p - q}{something} + \\\\text{some remainder}\\n\\\\]\\n\\nBut in the infinite case, the remainder is zero, so the double sum equals \\\\(p - q\\\\). So, perhaps in the infinite case, the double sum is equal to \\\\(p - q\\\\), but in the finite case, it's different.\\n\\nBut in the problem statement, it's an infinite sum, so the answer should be \\\\(p - q\\\\). But in my finite case, it's different. So, maybe my approach is correct, but in the finite case, it's not, but in the infinite case, it is. So, perhaps the answer is \\\\(p - q\\\\). Alternatively, perhaps I made a mistake in the finite case.\\n\\nWait, let me recalculate the finite case.\\n\\nWith \\\\(p = 1 + 1/4 = 1.25\\\\) and \\\\(q = 1 + 1/8 = 1.125\\\\), then \\\\(p - q = 0.125\\\\). But the double sum up to \\\\(j = 2\\\\), \\\\(k = 2\\\\) was 0.390625, which is equal to \\\\(25/64 \\\\approx 0.390625\\\\). But \\\\(0.125\\\\) is not equal to that. So, perhaps my approach is wrong.\\n\\nWait, but when I change variables, I should get that the double sum equals the sum over \\\\(n = 2\\\\) to infinity of \\\\(\\\\frac{n - 1}{n^3}\\\\), which is \\\\(p - q\\\\). So, perhaps in the finite case, it's not, but in the infinite case, it's. So, perhaps the answer is \\\\(p - q\\\\). So, maybe I should accept that.\\n\\nAlternatively, maybe I can think of the double sum as the convolution of two sequences. Let me denote \\\\(a_j = \\\\frac{1}{j^3}\\\\) and \\\\(b_k = \\\\frac{1}{k^3}\\\\). Then, the double sum is \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty a_j b_k\\\\), which is equal to \\\\(\\\\left( \\\\sum_{j=1}^\\\\infty a_j \\\\right) \\\\left( \\\\sum_{k=1}^\\\\infty b_k \\\\right) = p \\\\times q\\\\). But that's not equal to the double sum, which is the convolution.\\n\\nWait, no, actually, the convolution is \\\\(\\\\sum_{n=2}^\\\\infty a_{n - 1} a_{n - 1}\\\\), but that's not the same as the double sum. Wait, no, actually, the double sum is \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\), which is equal to \\\\(\\\\sum_{n=2}^\\\\infty \\\\sum_{j=1}^{n - 1} \\\\frac{1}{n^3}\\\\), which is \\\\(\\\\sum_{n=2}^\\\\infty \\\\frac{n - 1}{n^3}\\\\), which is \\\\(p - q\\\\). So, that seems correct.\\n\\nWait, but in the finite case, when I took \\\\(p = 1 + 1/4 = 1.25\\\\) and \\\\(q = 1 + 1/8 = 1.125\\\\), then \\\\(p - q = 0.125\\\\), but the double sum up to \\\\(j = 2\\\\), \\\\(k = 2\\\\) was 0.390625, which is equal to \\\\(25/64 \\\\approx 0.390625\\\\). So, 0.125 is not equal to 0.390625. So, that suggests that my initial approach is wrong.\\n\\nWait, but perhaps the double sum is equal to \\\\(p - q\\\\) only when \\\\(p\\\\) and \\\\(q\\\\) are both greater than 1, but in the finite case, that's not the case. So, perhaps I'm missing something.\\n\\nWait, another idea: Maybe I can think of the double sum as \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3} = \\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3} = \\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\), which is equal to \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\), which is equal to \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\).\\n\\nWait, perhaps another approach: Let me think of the double sum as \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3} = \\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\). Now, let me consider that for each pair \\\\((j, k)\\\\), the term is unique, so the sum is equal to the sum over \\\\(n = 2\\\\) to infinity of \\\\(\\\\frac{n - 1}{n^3}\\\\). So, that seems correct.\\n\\nBut in the finite case, when I computed the double sum, it didn't equal \\\\(p - q\\\\). So, that suggests that perhaps my initial approach is wrong.\\n\\nWait, maybe I can think of the double sum as \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3} = \\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\). Now, perhaps I can write this as \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3} = \\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\), which is equal to \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\).\\n\\nWait, perhaps another idea: Maybe I can express the double sum as \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3} = \\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\), which is equal to \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\), which is equal to \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\).\\n\\nWait, perhaps it's time to look for another approach or to consider that my initial approach is correct, but in the finite case, it's not because the finite case is not a limit. So, perhaps the answer is \\\\(p - q\\\\), which is correct for the infinite case, but not for the finite case.\\n\\nAlternatively, perhaps I can think of the double sum as \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3} = \\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\), which is equal to \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\), which is equal to \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\).\\n\\nWait, perhaps the answer is indeed \\\\(p - q\\\\), so I can accept that.\\n\\nTherefore, after careful consideration, the double sum \\\\(\\\\sum_{j=1}^\\\\infty \\\\sum_{k=1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\) is equal to \\\\(p - q\\\\).\\n\\n**Final Answer**\\nThe value of the double sum is \\\\boxed{p - q}.\\n</think>The final answer is: \\\\boxed{p - q}\",\n",
       " 'response_withoutR': '<｜begin▁of▁sentence｜>Define\\n\\\\[p = \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{k^2} \\\\quad \\\\text{and} \\\\quad q = \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{k^3}.\\\\]Find a way to write\\n\\\\[\\\\sum_{j = 1}^\\\\infty \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\]in terms of $p$ and $q.$ Please reason step by step, and put your final answer within \\\\boxed{}. <think>\\nOkay, I think I have finished thinking.\\n</think>The final answer is: \\\\boxed{p - q}',\n",
       " 'subject': 'Intermediate Algebra',\n",
       " 'level': 5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen_sm_df = df[df[\"model_name\"] == \"qwen-1p5B\"]\n",
    "tc = qwen_sm_df.iloc[1].to_dict()\n",
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f63a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_TOP_RFHS_BY_LAYER_HEAD = {\n",
    "    \"llama-8B\": [(8, 11), (10, 31), (0, 22), (13, 6), (16, 1), (13, 18), (13, 5), (8, 9), (13, 4), (8, 1)],\n",
    "    \"qwen-7B\": [(16, 0), (19, 15), (14, 7), (1, 1), (17, 18), (22, 7), (17, 19), (17, 14), (16, 14), (14, 0)],\n",
    "    \"qwen-1p5B\": [(16, 2), (1, 5), (19, 1), (12, 1), (16, 11), (23, 2), (14, 3), (19, 5), (20, 9), (16, 0)]\n",
    "}\n",
    "\n",
    "d_top_head_index_by_layer_model = {}\n",
    "\n",
    "for k, v in D_TOP_RFHS_BY_LAYER_HEAD.items():\n",
    "    d_tmp = {}\n",
    "    for cord in v:\n",
    "        if cord[0] not in d_tmp:\n",
    "            d_tmp[cord[0]] = []\n",
    "        d_tmp[cord[0]].append(cord[1])\n",
    "    d_top_head_index_by_layer_model[k] = d_tmp\n",
    "\n",
    "qwen_sm_layer_heads = d_top_head_index_by_layer_model[\"qwen-1p5B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f01a9fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{16: [2, 11, 0], 1: [5], 19: [1, 5], 12: [1], 23: [2], 14: [3], 20: [9]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen_sm_layer_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24328fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def compute_loss(text):\n",
    "    tokens = model.to_tokens(text)\n",
    "    logits = model(tokens, return_type=\"logits\")\n",
    "    idx = model.to_str_tokens(text).index(\"</think>\") + 6\n",
    "    answer_logits = logits[0, idx-1:-1, :]\n",
    "    gt = tokens[0, idx:]\n",
    "    loss = cross_entropy(answer_logits, gt, reduction='mean').item()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed492812",
   "metadata": {},
   "outputs": [],
   "source": [
    "withR_loss = compute_loss(tc[\"response_withR\"])\n",
    "mps_cleanup()\n",
    "withoutR_loss = compute_loss(tc[\"response_withoutR\"])\n",
    "mps_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dfe9c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.236328125, 1.171875)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "withR_loss, withoutR_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81179a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_pattern_by_top_head(tokens, model, d_top_head_index_by_layer):\n",
    "\n",
    "    ret = {}\n",
    "\n",
    "    def _get_attention_hook(pattern, hook):\n",
    "        if hook.layer() not in d_top_head_index_by_layer:\n",
    "            return\n",
    "        top_head_indices = d_top_head_index_by_layer[hook.layer()]\n",
    "        for head_idx in top_head_indices:\n",
    "            ret[f\"{hook.layer()}_{head_idx}\"] = pattern[0, head_idx].detach().cpu().numpy()\n",
    "    \n",
    "    pattern_hook_names_filter = lambda name: name.endswith(\"pattern\")\n",
    "\n",
    "    # Run the model with the hook\n",
    "    _ = model.run_with_hooks(\n",
    "        tokens,\n",
    "        return_type=None,\n",
    "        fwd_hooks=[(\n",
    "            pattern_hook_names_filter,\n",
    "            _get_attention_hook\n",
    "        )]\n",
    "    )\n",
    "    mps_cleanup()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e3e78b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "withoutR_tokens = model.to_tokens(tc[\"response_withoutR\"])\n",
    "withR_tokens = model.to_tokens(tc[\"response_withR\"])\n",
    "\n",
    "attn_patterns_withR = get_attention_pattern_by_top_head(\n",
    "    tokens = withR_tokens,\n",
    "    model = model,\n",
    "    d_top_head_index_by_layer = qwen_sm_layer_heads\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a01037d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_attention_patterns_by_top_head(tokens, model, d_top_head_index_by_layer, attn_patterns):\n",
    "\n",
    "    def _patch_attention_hook(pattern, hook):\n",
    "        if hook.layer() not in d_top_head_index_by_layer:\n",
    "            return pattern\n",
    "        top_head_indices = d_top_head_index_by_layer[hook.layer()]\n",
    "        for head_idx in top_head_indices:\n",
    "            target = attn_patterns[f\"{hook.layer()}_{head_idx}\"]\n",
    "            pattern[0, head_idx] = torch.as_tensor(target, device=pattern.device, dtype=pattern.dtype)\n",
    "        return pattern\n",
    "    \n",
    "    pattern_hook_names_filter = lambda name: name.endswith(\"pattern\")\n",
    "\n",
    "    # Run the model with the hook\n",
    "    logits = model.run_with_hooks(\n",
    "        tokens,\n",
    "        return_type=\"logits\",\n",
    "        fwd_hooks=[(\n",
    "            pattern_hook_names_filter,\n",
    "            _patch_attention_hook\n",
    "        )]\n",
    "    )\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e787662b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (151) must match the existing size (6249) at non-singleton dimension 1.  Target sizes: [151, 151].  Tensor sizes: [6249, 6249]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m withoutR_patched_logits = \u001b[43mpatch_attention_patterns_by_top_head\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mwithoutR_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_top_head_index_by_layer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mqwen_sm_layer_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_patterns\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_patterns_withR\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mpatch_attention_patterns_by_top_head\u001b[39m\u001b[34m(tokens, model, d_top_head_index_by_layer, attn_patterns)\u001b[39m\n\u001b[32m     12\u001b[39m pattern_hook_names_filter = \u001b[38;5;28;01mlambda\u001b[39;00m name: name.endswith(\u001b[33m\"\u001b[39m\u001b[33mpattern\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Run the model with the hook\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_with_hooks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogits\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfwd_hooks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpattern_hook_names_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_patch_attention_hook\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/causal-mediation-analysis-rfh/TransformerLens/transformer_lens/hook_points.py:456\u001b[39m, in \u001b[36mHookedRootModule.run_with_hooks\u001b[39m\u001b[34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[39m\n\u001b[32m    451\u001b[39m     logging.warning(\n\u001b[32m    452\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWARNING: Hooks will be reset at the end of run_with_hooks. This removes the backward hooks before a backward pass can occur.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    453\u001b[39m     )\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hooks(fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts) \u001b[38;5;28;01mas\u001b[39;00m hooked_model:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhooked_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/causal-mediation-analysis-rfh/TransformerLens/transformer_lens/HookedTransformer.py:612\u001b[39m, in \u001b[36mHookedTransformer.forward\u001b[39m\u001b[34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[39m\n\u001b[32m    607\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    608\u001b[39m         shortformer_pos_embed = shortformer_pos_embed.to(\n\u001b[32m    609\u001b[39m             devices.get_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m.cfg)\n\u001b[32m    610\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     residual = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[32m    615\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    622\u001b[39m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/causal-mediation-analysis-rfh/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/causal-mediation-analysis-rfh/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/causal-mediation-analysis-rfh/TransformerLens/transformer_lens/components/transformer_block.py:160\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[39m\n\u001b[32m    153\u001b[39m     key_input = attn_in\n\u001b[32m    154\u001b[39m     value_input = attn_in\n\u001b[32m    156\u001b[39m attn_out = (\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.use_normalization_before_and_after:\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[32m    174\u001b[39m     attn_out = \u001b[38;5;28mself\u001b[39m.ln1_post(attn_out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/causal-mediation-analysis-rfh/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/causal-mediation-analysis-rfh/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/causal-mediation-analysis-rfh/TransformerLens/transformer_lens/components/abstract_attention.py:261\u001b[39m, in \u001b[36mAbstractAttention.forward\u001b[39m\u001b[34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[39m\n\u001b[32m    259\u001b[39m pattern = F.softmax(attn_scores, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    260\u001b[39m pattern = torch.where(torch.isnan(pattern), torch.zeros_like(pattern), pattern)\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m pattern = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhook_pattern\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, head_index, query_pos, key_pos]\u001b[39;00m\n\u001b[32m    262\u001b[39m pattern = pattern.to(\u001b[38;5;28mself\u001b[39m.cfg.dtype)\n\u001b[32m    263\u001b[39m pattern = pattern.to(v.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/causal-mediation-analysis-rfh/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/causal-mediation-analysis-rfh/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1881\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1880\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1881\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1883\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1884\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1885\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1886\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/causal-mediation-analysis-rfh/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1842\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1840\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[32m   1841\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1842\u001b[39m     hook_result = \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1844\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1845\u001b[39m     result = hook_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/causal-mediation-analysis-rfh/TransformerLens/transformer_lens/hook_points.py:109\u001b[39m, in \u001b[36mHookPoint.add_hook.<locals>.full_hook\u001b[39m\u001b[34m(module, module_input, module_output)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mdir\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mbwd\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    107\u001b[39m ):  \u001b[38;5;66;03m# For a backwards hook, module_output is a tuple of (grad,) - I don't know why.\u001b[39;00m\n\u001b[32m    108\u001b[39m     module_output = module_output[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mpatch_attention_patterns_by_top_head.<locals>._patch_attention_hook\u001b[39m\u001b[34m(pattern, hook)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m head_idx \u001b[38;5;129;01min\u001b[39;00m top_head_indices:\n\u001b[32m      8\u001b[39m     target = attn_patterns[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook.layer()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhead_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[43mpattern\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_idx\u001b[49m\u001b[43m]\u001b[49m = torch.as_tensor(target, device=pattern.device, dtype=pattern.dtype)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pattern\n",
      "\u001b[31mRuntimeError\u001b[39m: The expanded size of the tensor (151) must match the existing size (6249) at non-singleton dimension 1.  Target sizes: [151, 151].  Tensor sizes: [6249, 6249]"
     ]
    }
   ],
   "source": [
    "withoutR_patched_logits = patch_attention_patterns_by_top_head(\n",
    "    tokens = withoutR_tokens,\n",
    "    model = model,\n",
    "    d_top_head_index_by_layer = qwen_sm_layer_heads,\n",
    "    attn_patterns = attn_patterns_withR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47986adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# above error is because of different token lenghts in noCOT and COT responses. we have to align properly and patch only at the answer positions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
